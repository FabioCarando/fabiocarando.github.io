# MusicGenerator

## Overview
MusicGenerator is a project designed to explore and implement music generation using neural networks. By leveraging deep learning techniques such as recurrent neural networks (RNNs) and Transformers, the project aims to create coherent and melodically pleasing musical sequences in MIDI format.

This repository includes data preprocessing tools, model implementations, and utilities to generate music based on trained models. The generated music can be evaluated both quantitatively and subjectively.

---

## Features
- Preprocessing tools for converting MIDI files into sequences suitable for model training.
- Model architectures including:
  - Long Short-Term Memory (LSTM) networks.
  - Transformer-based models for advanced music generation.
- Utilities to generate and save musical sequences in MIDI format.
- Easy-to-follow notebooks for data preprocessing, training, and generation.

---

## Installation

### Prerequisites
Ensure you have the following installed:
- Python 3.8+
- pip package manager

### Dependencies
Install the required packages using the provided `requirements.txt`:

```bash
pip install -r requirements.txt
```

### Required Libraries
The main dependencies are:
- TensorFlow/Keras (or PyTorch, if using alternative implementations)
- Music21
- PrettyMIDI
- Magenta
- NumPy
- Matplotlib

---

## Dataset

The primary dataset used is the [MAESTRO Dataset](https://magenta.tensorflow.org/datasets/maestro), which contains MIDI and audio files of piano performances.

### Downloading the Dataset
1. Visit the [MAESTRO Dataset page](https://magenta.tensorflow.org/datasets/maestro).
2. Download the MIDI files and place them in the `data/maestro` directory.
3. Use the preprocessing notebook to convert these files into a training-ready format.

---

## Project Structure
```
MusicGenerator/
  |
  |-- data/
  |    |-- maestro/         # Raw dataset files (MIDI)
  |    |-- processed/       # Processed data ready for training
  |
  |-- models/
  |    |-- lstm_model.py    # LSTM-based music generation model
  |    |-- transformer_model.py # Transformer-based music generation model
  |
  |-- notebooks/
  |    |-- data_preprocessing.ipynb  # Notebook for preprocessing MIDI files
  |    |-- training.ipynb            # Notebook for training the models
  |    |-- generation.ipynb          # Notebook for generating music
  |
  |-- requirements.txt       # Required Python packages
  |-- README.md              # Project documentation
```

---

## Usage

### Preprocessing MIDI Files
Use the `notebooks/data_preprocessing.ipynb` to:
1. Load raw MIDI files from the dataset.
2. Extract note sequences and store them in a processed format.

### Training a Model
1. Open `notebooks/training.ipynb`.
2. Configure the model parameters (LSTM or Transformer).
3. Train the model on the processed dataset.

### Generating Music
1. Use `notebooks/generation.ipynb`.
2. Provide a seed sequence to the trained model.
3. Generate music and save it as a MIDI file.

---

## Future Improvements
- Incorporate more advanced Transformer-based models for better contextual understanding.
- Expand to multi-instrument MIDI files.
- Develop a GUI to interactively generate music.

---

## Contributing
Contributions are welcome! If you want to add features or fix bugs, please:
1. Fork this repository.
2. Create a feature branch.
3. Submit a pull request.

---

## License
This project is licensed under the MIT License. See the `LICENSE` file for details.

---

## Acknowledgments
- [Magenta](https://magenta.tensorflow.org/) for inspiration and tools.
- The creators of the [MAESTRO Dataset](https://magenta.tensorflow.org/datasets/maestro).
- TensorFlow and PyTorch communities for providing robust frameworks.


